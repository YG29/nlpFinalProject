{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T11:44:22.776701Z",
     "start_time": "2024-03-23T11:44:22.755491Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (encoder.py, line 13)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001B[0;36m(most recent call last)\u001B[0m:\n",
      "\u001B[0m  File \u001B[1;32m~/Library/Caches/pypoetry/virtualenvs/nlpfinalproject-E4blksoP-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001B[0m in \u001B[1;35mrun_code\u001B[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001B[0m\n",
      "\u001B[0;36m  Cell \u001B[0;32mIn[26], line 5\u001B[0;36m\n\u001B[0;31m    from encoder import Encoder\u001B[0;36m\n",
      "\u001B[0;36m  File \u001B[0;32m~/Documents/alexandria/RUG/ml/nlpProject/notebooks/../src/encoder.py:13\u001B[0;36m\u001B[0m\n\u001B[0;31m    Dense(hidden_dimension, activation = 'relu')\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from positionalencoding import PositionalEncoding\n",
    "from encoder import Encoder\n",
    "from transformer import Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:57:24.401112Z",
     "start_time": "2024-03-22T13:57:24.388328Z"
    }
   },
   "id": "88388155ae3bb66"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "TRAIN_EN = '../data/clean/en_train_padded.csv'\n",
    "TRAIN_NL = '../data/clean/nl_train_padded.csv'\n",
    "VAL_EN = '../data/clean/en_val_padded.csv'\n",
    "VAL_NL = '../data/clean/nl_val_padded.csv'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENCE = 50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T11:20:29.721587Z",
     "start_time": "2024-03-23T11:20:29.711457Z"
    }
   },
   "id": "cd92740effc6f71b"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "#load and preprocess\n",
    "def load_preprocess(input_file_path, target_file_path, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    # load data from csv\n",
    "    input_data = pd.read_csv(input_file_path)['input_data'].tolist()\n",
    "    target_data = pd.read_csv(target_file_path)['target_data'].tolist()\n",
    "    \n",
    "    # create tensorflow dataset\n",
    "    input_dataset = tf.data.Dataset.from_tensor_slices(input_data)\n",
    "    target_dataset = tf.data.Dataset.from_tensor_slices(target_data)\n",
    "    \n",
    "    # zip input and target together\n",
    "    dataset = tf.data.Dataset.zip((input_dataset, target_dataset))\n",
    "    \n",
    "    # processing function\n",
    "    def preprocess(dat_input, dat_target):\n",
    "        dat_input = tf.cast(dat_input, tf.int64)\n",
    "        dat_target = tf.cast(dat_target, tf.int64)\n",
    "        return dat_input, dat_target\n",
    "    # apply processing function\n",
    "    dataset = dataset.map(preprocess)\n",
    "    \n",
    "    # batch and prefetch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # calculate dataset length\n",
    "    num_samples = len(input_data)\n",
    "    print(num_samples)\n",
    "    \n",
    "    # wrap dataset with tqdm for progress bar\n",
    "    dataset = tqdm(dataset, total=num_samples // batch_size)\n",
    "    \n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T11:46:18.012095Z",
     "start_time": "2024-03-23T11:46:18.004775Z"
    }
   },
   "id": "1b61c52f3b91fba1"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_preprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_EN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mTRAIN_NL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m load_preprocess(VAL_EN,VAL_NL)\n",
      "Cell \u001B[0;32mIn[47], line 5\u001B[0m, in \u001B[0;36mload_preprocess\u001B[0;34m(input_file_path, target_file_path, batch_size)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_preprocess\u001B[39m(input_file_path, target_file_path, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE):\n\u001B[1;32m      3\u001B[0m     \n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# load data from csv\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m     input_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_file_path\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_data\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      6\u001B[0m     target_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(target_file_path)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_data\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# create tensorflow dataset\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nlpfinalproject-E4blksoP-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nlpfinalproject-E4blksoP-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nlpfinalproject-E4blksoP-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nlpfinalproject-E4blksoP-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1895\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m   1897\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1898\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1899\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1900\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nlpfinalproject-E4blksoP-py3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype_backend\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;66;03m# Fail here loudly instead of in cython after reading\u001B[39;00m\n\u001B[1;32m     92\u001B[0m     import_optional_dependency(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 93\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m \u001B[43mparsers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munnamed_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39munnamed_cols\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "File \u001B[0;32mparsers.pyx:574\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:721\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._get_header\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = load_preprocess(TRAIN_EN,TRAIN_NL)\n",
    "val_dataset = load_preprocess(VAL_EN,VAL_NL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T11:49:44.472261Z",
     "start_time": "2024-03-23T11:46:19.333836Z"
    }
   },
   "id": "7421067302ad9953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1a2753c64af78ae0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_mask(input, target):\n",
    "    # encoding mask\n",
    "    encoding_padding_mask = tf.cast(tf.math.equal(input, 0), tf.float32)\n",
    "    encoding_padding_mask = encoding_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    # decoding mask\n",
    "    decoding_padding_mask = tf.cast(tf.math.equal(target, 0), tf.float32)\n",
    "    decoding_padding_mask = decoding_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    # future mask\n",
    "    future_mask = tf.linalg.band_part(tf.ones((1, None, None, None)), -1, 0)\n",
    "    future_mask = tf.maximum(decoding_padding_mask, future_mask)\n",
    "    \n",
    "    return encoding_padding_mask, future_mask, decoding_padding_mask"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54f20802d8065a03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_layers = 2\n",
    "embedding_dimension = 10\n",
    "num_heads = 4\n",
    "ff_dimension = 4\n",
    "input_vocab_size = 900\n",
    "target_vocab_size = 900\n",
    "learning_rate = 0.2\n",
    "num_epochs = 200"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T11:43:45.480013Z"
    }
   },
   "id": "37afd92a7963d9c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model set up\n",
    "model = Transformer(num_layers, embedding_dimension, num_heads, ff_dimension, input_vocab_size, target_vocab_size, max_len_input=MAX_SENTENCE, max_len_output=MAX_SENTENCE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0e73599ff7fb2f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataset:\n",
    "        # create the masks\n",
    "        encoding_padding_mask, future_mask, decoding_padding_mask = create_mask(inputs,targets)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _, _ = model(inputs, targets, True, encoding_padding_mask, future_mask, decoding_padding_mask)\n",
    "            loss = loss_object(targets, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    # validation loop\n",
    "    total_val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    for inputs, targets in val_dataset:\n",
    "        encoding_padding_mask, future_mask, decoding_padding_mask = create_mask(inputs, targets)\n",
    "        predictions, _, _ = model(inputs, targets, False, encoding_padding_mask, future_mask, decoding_padding_mask)\n",
    "        val_loss = loss_object(targets, predictions)\n",
    "        total_val_loss += val_loss.np().sum()\n",
    "        num_val_batches += 1\n",
    "        \n",
    "    # print average validation loss\n",
    "    ave_val_loss = total_val_loss / num_val_batches\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {ave_val_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T11:43:45.485018Z",
     "start_time": "2024-03-23T11:43:45.484887Z"
    }
   },
   "id": "3743a2d3d9ed19d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c4dffa03baa56d02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
